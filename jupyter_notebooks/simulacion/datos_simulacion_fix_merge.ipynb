{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stats() missing 2 required positional arguments: 'lead_times_dict' and 'S_factor_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fb9296ee0718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[0mstats_no_agrupados\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatos_limpios_HUB_SKUS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.65\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlead_times_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS_factor_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[0msS_dict_no_agrupados\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_sS_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats_no_agrupados\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m \u001b[0mstats_agrupados\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_grouped_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatos_limpios_HUB_SKUS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.65\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhubs_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[0msS_dict_agrupados\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_sS_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats_agrupados\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-fb9296ee0718>\u001b[0m in \u001b[0;36mcreate_grouped_dataframe\u001b[1;34m(dataframe, confianza, hubs_dict)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_grouped_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfianza\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhubs_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mstats_no_agrupados\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfianza\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m     \u001b[0massign_hub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhubs_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DESCR_CENDIST'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: stats() missing 2 required positional arguments: 'lead_times_dict' and 'S_factor_dict'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "hubs_dict = {\n",
    "    'ARICA': 'IQUIQUE',\n",
    "    'IQUIQUE': 'IQUIQUE',\n",
    "    'ANTOFAGASTA': 'ANTOFAGASTA',\n",
    "    'COPIAPO': 'COPIAPO',\n",
    "    'COQUIMBO': 'COQUIMBO',\n",
    "    'OVALLE': 'COQUIMBO',\n",
    "    'ILLAPEL': 'CURAUMA',\n",
    "    'LLAY LLAY': 'CURAUMA',\n",
    "    'CURAUMA': 'CURAUMA',\n",
    "    'SANTIAGO SUR': 'SANTIAGO SUR',\n",
    "    'RANCAGUA': 'SANTIAGO SUR',\n",
    "    'TALCA': 'TALCA',\n",
    "    'CHILLAN': 'TALCAHUANO',\n",
    "    'TALCAHUANO': 'TALCAHUANO',\n",
    "    'LOS ANGELES': 'TALCAHUANO',\n",
    "    'TEMUCO': 'TEMUCO',\n",
    "    'VALDIVIA': 'VALDIVIA',\n",
    "    'OSORNO': 'VALDIVIA',\n",
    "    'PUERTO MONTT': 'PUERTO MONTT',\n",
    "    'CASTRO': 'PUERTO MONTT',\n",
    "    'COYHAIQUE': 'COYHAIQUE',\n",
    "    'CALAMA': 'ANTOFAGASTA'\n",
    "}\n",
    "\n",
    "with open('lead_times_dict.json', 'r') as file:\n",
    "    lead_times_dict = json.load(file)\n",
    "    \n",
    "with open('S_factor_dict.json', 'r') as file:\n",
    "    S_factor_dict = json.load(file)\n",
    "\n",
    "def create_filtered_dataframe(file, HUB, n_skus):\n",
    "    datos_limpios = pd.read_csv(file)\n",
    "    datos_limpios = datos_limpios.drop(columns='Unnamed: 0')\n",
    "    datos_limpios_HUB = datos_limpios[datos_limpios['HUB'] == HUB]\n",
    "\n",
    "    top_skus = datos_limpios_HUB.groupby(by=['ID_SKU_VENTA']).sum().reset_index()\n",
    "    top_n_skus = top_skus.sort_values(by='Venta en pallets', ascending=False).head(n_skus)\n",
    "    top_n_skus_list = list(top_n_skus['ID_SKU_VENTA'])\n",
    "\n",
    "    datos_limpios_HUB_SKUS = datos_limpios_HUB[datos_limpios_HUB['ID_SKU_VENTA'].isin(top_n_skus_list)]\n",
    "    \n",
    "    return datos_limpios_HUB_SKUS\n",
    "\n",
    "\n",
    "def simulation_generator(dataframe, sS_dict, sku, cd):\n",
    "    \n",
    "    sS_info = sS_dict[(sku, cd)]\n",
    "    \n",
    "    dataframe = dataframe[(dataframe['ID_SKU_VENTA'] == sku) & (dataframe['DESCR_CENDIST'] == cd)]\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        yield row['Venta en pallets'], sS_info[0], sS_info[1]\n",
    "        \n",
    "def stats(dataframe, confianza, lead_times_dict, S_factor_dict):\n",
    "\n",
    "    mean_no_group = dataframe.groupby(by=['ID_SKU_VENTA', 'DESCR_CENDIST']).mean().reset_index().rename(columns={'Venta en pallets': 'MEDIA'})\n",
    "    stdev_no_group = dataframe.groupby(by=['ID_SKU_VENTA', 'DESCR_CENDIST']).std().reset_index().rename(columns={'Venta en pallets': 'STD'})\n",
    "    \n",
    "    mean_no_group['MEDIA'] = mean_no_group.apply(lambda x: x.MEDIA * lead_times_dict[x['DESCR_CENDIST']], axis=1)\n",
    "    stdev_no_group['STD'] = stdev_no_group.apply(lambda x: x.STD * lead_times_dict[x['DESCR_CENDIST']], axis=1)\n",
    "\n",
    "    data_completa = mean_no_group.merge(stdev_no_group, on=['ID_SKU_VENTA', 'DESCR_CENDIST'])\n",
    "    \n",
    "    data_completa['SS'] = data_completa.apply(lambda x: confianza * x['STD'], axis=1)\n",
    "\n",
    "    data_completa['s'] = data_completa.apply(lambda x: x['MEDIA'] + (confianza * x['STD']), axis=1)\n",
    "    \n",
    "    data_completa['S'] = data_completa.apply(lambda x: x['s'] * int(S_factor_dict[str(x['ID_SKU_VENTA'])]), axis=1)\n",
    "    \n",
    "    return data_completa.drop(columns=['Unnamed: 0.1_x', 'Unnamed: 0.1_y'])\n",
    "\n",
    "\n",
    "def create_sS_dict(dataframe):\n",
    "    \n",
    "    sS_dict = {}\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        sS_dict[(row['ID_SKU_VENTA'], row['DESCR_CENDIST'])] = (row['s'], row['S'])\n",
    "        \n",
    "    return sS_dict\n",
    "\n",
    "def create_grouped_dataframe(dataframe, confianza, hubs_dict, lead_times_dict, S_factor_dict):\n",
    "    stats_no_agrupados = stats(dataframe, confianza, lead_times_dict, S_factor_dict)\n",
    "    assign_hub = lambda x: hubs_dict[x['DESCR_CENDIST']]\n",
    "    \n",
    "    datos_para_agrupar = dataframe.copy()\n",
    "    datos_para_agrupar['DESCR_CENDIST'] = datos_para_agrupar.apply(assign_hub, axis=1)\n",
    "    datos_agrupados = datos_para_agrupar.groupby(by=['ID_SKU_VENTA', 'DESCR_CENDIST', 'FECHA']).sum().reset_index()\n",
    "    \n",
    "    stats_agrupados = stats(datos_agrupados, confianza, lead_times_dict, S_factor_dict)\n",
    "    \n",
    "    todos = stats_no_agrupados.merge(stats_agrupados, on=['ID_SKU_VENTA', 'DESCR_CENDIST'], how='outer').fillna(0)\n",
    "    \n",
    "    todos.rename(columns={'MEDIA_x': 'MEDIA_NO_AGR', 'STD_x': 'STD_NO_AGR', 'STD_y': 'STD_AGR', 'SS_x': 'SS_NO_AGR', 'SS_y': 'SS_AGR'}, inplace=True)\n",
    "\n",
    "    todos = todos[['ID_SKU_VENTA', 'DESCR_CENDIST', 'MEDIA_NO_AGR', 'STD_NO_AGR', 'STD_AGR', 'SS_NO_AGR', 'SS_AGR']]\n",
    "    \n",
    "    todos.drop(columns=['STD_NO_AGR', 'STD_AGR', 'SS_NO_AGR'], inplace=True)\n",
    "    \n",
    "    todos['s'] = todos.apply(lambda x: x['MEDIA_NO_AGR'] + x['SS_AGR'], axis=1)\n",
    "    \n",
    "    todos['S'] = todos.apply(lambda x: x['s'] * int(S_factor_dict[str(x['ID_SKU_VENTA'])]), axis=1)\n",
    "    \n",
    "    return todos[['ID_SKU_VENTA', 'DESCR_CENDIST', 's', 'S']]\n",
    "        \n",
    "datos_limpios_HUB_SKUS = create_filtered_dataframe('../../data/datos_limpios_filtrados.csv', 'TALCAHUANO', 10) \n",
    "stats_no_agrupados = stats(datos_limpios_HUB_SKUS, 1.65, lead_times_dict, S_factor_dict)\n",
    "sS_dict_no_agrupados = create_sS_dict(stats_no_agrupados)\n",
    "stats_agrupados = create_grouped_dataframe(datos_limpios_HUB_SKUS, 1.65, hubs_dict, lead_times_dict, S_factor_dict)\n",
    "sS_dict_agrupados = create_sS_dict(stats_agrupados)\n",
    "\n",
    "generador_caso_base = simulation_generator(datos_limpios_HUB_SKUS, sS_dict_no_agrupados, 450607, 'TALCAHUANO')\n",
    "generador_caso_proyecto = simulation_generator(datos_limpios_HUB_SKUS, sS_dict_agrupados, 450607, 'TALCAHUANO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
